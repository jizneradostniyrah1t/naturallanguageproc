{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36dcec56-bcfe-4932-9704-5c86b9a0aed9",
   "metadata": {},
   "source": [
    "<H1><center>–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞</center></H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb46426-ca1e-4546-8b76-1065af74d4a5",
   "metadata": {},
   "source": [
    "<H2>–í–≤–µ–¥–µ–Ω–∏–µ</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f5ca8f-990d-4b02-945d-54c76dfd4f47",
   "metadata": {},
   "source": [
    "–í –¥–∞–Ω–Ω–æ–π –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –æ–¥–Ω—É –∏–∑ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ - rubert-tiny  \n",
    "Rubert-tiny —è–≤–ª—è–µ—Ç—Å—è —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π BERT, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤—Å–µ–≥–æ –Ω–∞ 2 —è–∑—ã–∫–∞: —Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π (–æ—Ç—Å—é–¥–∞ –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da5d33-b38d-49be-a070-9fd71f43da31",
   "metadata": {},
   "source": [
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ –≤–∞–º –±—É–¥–µ—Ç –¥–∞–Ω –∫–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ CoLA (Corpus of Linguistic Acceptability)  \n",
    "–í—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –≤ —ç—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —è–≤–ª—è—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, –∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∏—Ö –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15fbaa3-bbe9-445a-b615-193d6c696304",
   "metadata": {},
   "source": [
    "–ó–∞—Ç–µ–º –≤—ã –¥–æ–ª–∂–Ω—ã –±—É–¥–µ—Ç–µ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –æ–±—É—á–∏—Ç—å —ç—Ç—É –∂–µ (–º–æ–∂–Ω–æ –¥—Ä—É–≥—É—é) –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å –æ—Ç–∑—ã–≤–∞–º–∏ –∫ —Ñ–∏–ª—å–º–∞–º  \n",
    "–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —Ç–∏–ø –æ—Ç–∑—ã–≤–∞ - –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f92b8d-01c7-4191-b791-a92ec4bbdd60",
   "metadata": {},
   "source": [
    "<H4>–¶–µ–ª–∏</H4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8a3a9-9fdf-47ec-b23b-1bdb15b4c1b5",
   "metadata": {},
   "source": [
    "- –ò–∑—É—á–∏—Ç—å –ø—Ä–∏–Ω—Ü–∏–ø—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞\n",
    "- –û–±—É—á–∏—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å rubert-tiny –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —Ç–∏–ø –æ—Ç–∑—ã–≤–∞ –Ω–∞ —Ñ–∏–ª—å–º (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π/–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ad922-0dec-4ed2-98aa-766e4e85ef1b",
   "metadata": {},
   "source": [
    "<H4>–ó–∞–¥–∞—á–∏</H4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa457897-ad82-415f-952d-09de8101828d",
   "metadata": {},
   "source": [
    "<H4>–û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è</H4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47278dd6-4207-4b54-be7c-7362d9bd2117",
   "metadata": {},
   "source": [
    "\\[1] ‚≠ê - –ó–∞–¥–∞–Ω–∏–µ (–∑–≤–µ–∑–¥–æ—á–∫–∏ - –±–∞–ª–ª—ã, —á–∏—Å–ª–æ - –Ω–æ–º–µ—Ä –∑–∞–¥–∞–Ω–∏—è)  \n",
    "\\[1] üí´ - –ö–æ–Ω–µ—Ü –∑–∞–¥–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356ac3f-2094-472e-8a1e-1e3deee59a48",
   "metadata": {},
   "source": [
    "<H2>–û–±—É—á–µ–Ω–∏–µ rubert-tiny –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ CoLA</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1ab53-8d4b-44b3-923f-826345f8ec73",
   "metadata": {},
   "source": [
    "<H3>1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bdfacd-277f-4f4b-bdd1-aabcd243e233",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –æ–±—ã—á–Ω–æ, –Ω–∞—à–∞ –ø–µ—Ä–≤–∞—è –∑–∞–¥–∞—á–∞ - –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è  \n",
    "–°—á–∏—Ç–∞–µ–º —Ç–∞–±–ª–∏—Ü—É \"—Å—ã—Ä—ã—Ö\" —Ç–µ–∫—Å—Ç–æ–≤ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –≤ –Ω–µ–π –ª–µ–∂–∏—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4074f5ce-b3ae-45fd-a571-4c0f0cd61c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º dataset –≤ pandas dataframe.\n",
    "df = pd.read_csv(\"./data/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "sentences = df['sentence'].values\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49aae3e4-f4c3-4e14-808a-8890973a058f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 8,551\n",
      "\n",
      "     sentence_source  label label_notes  \\\n",
      "87              gj04      1         NaN   \n",
      "1483            r-67      1         NaN   \n",
      "4610            ks08      1         NaN   \n",
      "6279            c_13      1         NaN   \n",
      "4559            ks08      0           *   \n",
      "3131            l-93      1         NaN   \n",
      "6544            g_81      0           *   \n",
      "6122            c_13      1         NaN   \n",
      "6042            c_13      0           *   \n",
      "6412            d_98      1         NaN   \n",
      "\n",
      "                                               sentence  \n",
      "87                             The tiger bled to death.  \n",
      "1483       A review came out yesterday of this article.  \n",
      "4610             John has chosen Bill for the position.  \n",
      "6279         To improve myself is a goal for next year.  \n",
      "4559          It has rains every day for the last week.  \n",
      "3131                                      Paul exhaled.  \n",
      "6544  The table, I put Kim on which supported the book.  \n",
      "6122                                    She was kissed.  \n",
      "6042                       I wanted if he should leave.  \n",
      "6412  Every restaurant that advertises in any of the...  \n",
      "                                               sentence  label\n",
      "3432                           Man the ball kicked the.      0\n",
      "2516              This list includes my name on itself.      0\n",
      "1455  Her efficient looking of the answer up pleased...      0\n",
      "1471  Which packages is it possible that Sam didn't ...      0\n",
      "23                              We yelled Harry hoarse.      0\n"
     ]
    }
   ],
   "source": [
    "# –í—ã–≤–æ–¥–∏–º —á–∏—Å–ª–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Å–ª—É—á–∞–π–Ω—ã–µ 10 —Ä—è–¥–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—á–∫–∏.\n",
    "print(df.sample(10))\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º 5 –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–≤–µ—Ä–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.\n",
    "print(df.loc[df.label == 0].sample(5)[['sentence', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ffa23-4521-4f12-a72a-aa42ac2e09af",
   "metadata": {},
   "source": [
    "<br>\n",
    "<H4>1.1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è</H4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041f0ca-39de-4d26-998d-1cb07a310fa1",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –≤—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω—É–∂–Ω–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —Ñ–æ—Ä–º–∞—Ç—É, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –ø—Ä–æ—á–∏—Ç–∞—Ç—å —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å  \n",
    "–í —ç—Ç–æ–º –Ω–∞–º –ø–æ–º–æ–∂–µ—Ç –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7ba46c-3640-4bdf-8500-12aa81c888e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./models/rubert-tiny', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde13886-0dc4-4845-9df8-cc5c40ab3a20",
   "metadata": {},
   "source": [
    "<br>–°–Ω–∞—á–∞–ª–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —á–∞—Å—Ç–∏ (—Ç–æ–∫–µ–Ω—ã), –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –µ–≥–æ —Å–ª–æ–≤–∞—Ä–µ  \n",
    "–¢–æ–∫–µ–Ω–æ–º –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–≤–æ, —á–∞—Å—Ç—å —Å–ª–æ–≤–∞ –∏–ª–∏ —Å–∏–º–≤–æ–ª, –∞ —Ç–∞–∫–∂–µ –æ–¥–Ω–æ –∏–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13bd4b2a-8ccf-45a4-829f-d79f0422bbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[0]\n",
    "print('Original:', sentence)                                 # –í—ã–≤–µ—Å—Ç–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentence))           # –í—ã–≤–µ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, —Ä–∞–∑–±–∏—Ç–æ–µ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ —Å–ª–æ–≤–∞—Ä—è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20731552-edde-4dc3-9cbb-1b8c906b8dfc",
   "metadata": {},
   "source": [
    "<br>–°–ª–æ–≤–∞—Ä—å –Ω–∞—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ñ–∞–π–ª–µ ./models/rubert-tiny/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311ea50-5001-42c6-a0f8-705742415c18",
   "metadata": {},
   "source": [
    "<center>[1] ‚≠ê</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1f30d-75a7-4f5d-b9ae-b4f7ffb7f393",
   "metadata": {},
   "source": [
    "–û—Ç–∫—Ä–æ–π—Ç–µ —Å–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –Ω–∞–π–¥–∏—Ç–µ –≤ –Ω–µ–º:\n",
    "- —Ä—É—Å—Å–∫–æ–µ —Å–ª–æ–≤–æ\n",
    "- —á–∞—Å—Ç—å —Ä—É—Å—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞\n",
    "- —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω\n",
    "\n",
    "‚≠ê –û–±—ä—è—Å–Ω–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞  \n",
    "\n",
    "P.S. –í—Å–µ 3 —Ç–æ–∫–µ–Ω–∞ –¥–æ–ª–∂–Ω—ã –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤–∞—à–∏—Ö —Å–æ—Å–µ–¥–µ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f098e17-4bd9-4cb7-8d51-4601a88ec19e",
   "metadata": {},
   "source": [
    "–∂—É—Ä–Ω–∞–ª–∞\n",
    "–¥–∑–µ\n",
    "[CLS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336904ac-18ce-4889-b08f-a833bcc35847",
   "metadata": {},
   "source": [
    "<center>[1] üí´</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734bdc0d-f4d2-4108-9e27-21ee74b9ab4f",
   "metadata": {},
   "source": [
    "–¢–æ–∫–µ–Ω—ã –∑–∞—Ç–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –≤ –∏—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –º–æ–∂–Ω–æ –ø–æ–¥–∞–≤–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873d34bc-a688-46cc-bdfd-feccecd88e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:  [4589, 6597, 1427, 11, 87, 15085, 881, 5356, 16, 2703, 7421, 531, 2624, 835, 1798, 9529, 18]\n"
     ]
    }
   ],
   "source": [
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence)))       # –í—ã–≤–µ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, —Ä–∞–∑–±–∏—Ç–æ–µ –Ω–∞ –Ω–æ–º–µ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29638ec7-963c-4bd0-bd81-fc1a49f948c7",
   "metadata": {},
   "source": [
    "<br>–¢–æ –∂–µ —Å–∞–º–æ–µ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤ –æ–¥–∏–Ω —à–∞–≥, –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ed818d-e013-4117-9d42-cee866b7c9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      "tensor([[   2, 4589, 6597,  ...,    0,    0,    0],\n",
      "        [   2,  835, 1052,  ...,    0,    0,    0],\n",
      "        [   2,  835, 1052,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  683,  550,  ...,    0,    0,    0],\n",
      "        [   2,   76,  768,  ...,    0,    0,    0],\n",
      "        [   2, 2376,  813,  ...,    0,    0,    0]])\n",
      "Attention masks:\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "encoded_dict = tokenizer.batch_encode_plus(\n",
    "    list(sentences),                           # –¢–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.\n",
    "    add_special_tokens=True,                   # –î–æ–±–∞–≤–ª—è–µ–º '[CLS]' –∏ '[SEP]'\n",
    "    max_length=64,                             # –î–æ–ø–æ–ª–Ω—è–µ–º [PAD] –∏–ª–∏ –æ–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ 64 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–∞–∫–∂–µ attn. masks.\n",
    "    return_tensors='pt',                       # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –≤–∏–¥–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤ pytorch.\n",
    ")\n",
    "\n",
    "print(f'Token IDs:\\n{encoded_dict[\"input_ids\"]}')\n",
    "print(f'Attention masks:\\n{encoded_dict[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f306ec0-fa51-42d0-a19d-fd0dfc1d47ef",
   "metadata": {},
   "source": [
    "<center>[2] ‚≠ê</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5ec092-c2d7-42b7-abbc-10144cbccb3b",
   "metadata": {},
   "source": [
    "–ó–∞–∫–æ–¥–∏—Ä—É–π—Ç–µ —Å–≤–æ–π –æ—Ç–∑—ã–≤ –Ω–∞ —Ñ–∏–ª—å–º —Å –ø–æ–º–æ—â—å—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞  \n",
    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ <code>encode_plus</code> –≤–º–µ—Å—Ç–æ <code>batch_encode_plus</code>, —á—Ç–æ–±—ã –ø–µ—Ä–µ–¥–∞—Ç—å –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç, –∞ –Ω–µ —Å–ø–∏—Å–æ–∫\n",
    "\n",
    "–í—ã–≤–µ–¥–∏—Ç–µ —Ç–æ–∫–µ–Ω—ã (–Ω–µ –∏—Ö id) —Å –ø–æ–º–æ—â—å—é <code>tokenize</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f91e3f6b-45c3-4b06-8204-783702a63dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['–∑–∞', '##–º–µ', '##—á–∞—Ç', '##–µ–ª—å', '##–Ω—ã', '##–∏', '–∫', '##—Ä–∏–º', '##–∏–Ω–∞–ª', '##—å–Ω—ã', '##–∏', '—Ñ–∏–ª—å–º', '-', '–∫–æ–º–µ', '##–¥–∏—è', '.', '–ø–æ', '—Å–≤–æ–µ–º—É', '–≤—ã', '##–¥–∞', '##—é', '##—â–∏', '##–∏—Å—è', ',', '–º–µ—Å—Ç–∞', '##–º–∏', '–∑–∞', '##–ø–æ–º', '##–∏–Ω–∞', '##—é', '##—â–∏', '##–∏—Å—è', ',', '—ç—Ç–æ—Ç', '—Ñ–∏–ª—å–º', '–Ω–µ', '—Ä–æ–º–∞–Ω', '##—Ç–∏', '##–∑–∏', '##—Ä—É–µ—Ç', '–æ–±—Ä–∞–∑', '–ø—Ä–µ', '##—Å—Ç—É–ø', '##–Ω–∏–∫–æ–≤', ',', '–Ω–∞', '##–æ–±', '##–æ—Ä–æ—Ç', '–æ–±–ª', '##–∏—á–∞', '##–µ—Ç', ',', '—Ö–æ—Ç', '##—å', '–∏', '–≤', '–∏', '##—Ä–æ–Ω–∏', '##—á–Ω–æ', '##–∏', '—Ñ–æ—Ä–º–µ', '.']\n",
      "Token IDs:\n",
      "tensor([[   2, 4589, 6597,  ...,    0,    0,    0],\n",
      "        [   2,  835, 1052,  ...,    0,    0,    0],\n",
      "        [   2,  835, 1052,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  683,  550,  ...,    0,    0,    0],\n",
      "        [   2,   76,  768,  ...,    0,    0,    0],\n",
      "        [   2, 2376,  813,  ...,    0,    0,    0]])\n",
      "Attention masks:\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "rew = \"–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π –∫—Ä–∏–º–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∏–ª—å–º-–∫–æ–º–µ–¥–∏—è. –ü–æ —Å–≤–æ–µ–º—É –≤—ã–¥–∞—é—â–∏–π—Å—è, –º–µ—Å—Ç–∞–º–∏ –∑–∞–ø–æ–º–∏–Ω–∞—é—â–∏–π—Å—è, —ç—Ç–æ—Ç —Ñ–∏–ª—å–º –Ω–µ —Ä–æ–º–∞–Ω—Ç–∏–∑–∏—Ä—É–µ—Ç –æ–±—Ä–∞–∑ –ø—Ä–µ—Å—Ç—É–ø–Ω–∏–∫–æ–≤, –Ω–∞–æ–±–æ—Ä–æ—Ç –æ–±–ª–∏—á–∞–µ—Ç, —Ö–æ—Ç—å –∏ –≤ –∏—Ä–æ–Ω–∏—á–Ω–æ–π —Ñ–æ—Ä–º–µ.\"\n",
    "\n",
    "encoded_dict_rew = tokenizer.encode_plus(\n",
    "    list(rew),                           # –¢–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.\n",
    "    add_special_tokens=True,                   # –î–æ–±–∞–≤–ª—è–µ–º '[CLS]' –∏ '[SEP]'\n",
    "    max_length=64,                             # –î–æ–ø–æ–ª–Ω—è–µ–º [PAD] –∏–ª–∏ –æ–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ 64 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–∞–∫–∂–µ attn. masks.\n",
    "    return_tensors='pt',                       # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –≤–∏–¥–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤ pytorch.\n",
    ")\n",
    "\n",
    "print(tokenizer.tokenize(rew))\n",
    "print(f'Token IDs:\\n{encoded_dict[\"input_ids\"]}')\n",
    "print(f'Attention masks:\\n{encoded_dict[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91ff223-8a2d-4fdf-87cc-a79d891655b4",
   "metadata": {},
   "source": [
    "<center>[2] üí´</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24faba97-45cd-4799-9382-c6d3e0205736",
   "metadata": {},
   "source": [
    "<H4>1.2. –ó–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö</H4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c72c87f-cb86-41d1-835e-824ff4125832",
   "metadata": {},
   "source": [
    "–ò—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø–æ –¥–∞–Ω–Ω—ã–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –±—É–¥–µ–º —Å –ø–æ–º–æ—â—å—é —É–∂–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –Ω–∞–º –æ–±—ä–µ–∫—Ç–æ–≤ <code>DataLoader</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e5126e-cc11-42bb-b3a3-c90876a43f12",
   "metadata": {},
   "source": [
    "–î–ª—è –Ω–∞—á–∞–ª–∞ —Å–æ–±–µ—Ä–µ–º –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ –≤ –∫–ª–∞—Å—Å <code>TensorDataset</code>  \n",
    "–ù–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è (—á—Ç–æ–±—ã –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—É—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã) –∏ –≤—ã—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754237df-5260-41bf-921b-9ca410378da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "input_ids = encoded_dict['input_ids']\n",
    "attention_masks = encoded_dict['attention_mask']\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149ccf4e-8208-4dbb-b645-71976cecd5b3",
   "metadata": {},
   "source": [
    "<br>–¢–µ–ø–µ—Ä—å –ø–æ–¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –≤ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ 90/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c927fa0b-6c4b-4896-98a9-791738a121eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,695 training samples\n",
      "  856 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# –°—á–∏—Ç–∞–µ–º —á–∏—Å–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# –†–∞–∑–±–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å —É—á–µ—Ç–æ–º –ø–æ—Å—á–∏—Ç–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6fe9a-37c1-4734-b94c-9f905ec86e61",
   "metadata": {},
   "source": [
    "<br>–°–æ–∑–¥–∞–¥–∏–º —Å–≤–æ–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7f8e56d-4a6a-4561-9ffe-ae85b9edb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# –ó–∞–¥–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏.\n",
    "# –ê–≤—Ç–æ—Ä—ã BERT –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç–∞–≤–∏—Ç—å –µ–≥–æ 16 –∏–ª–∏ 32. \n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,              # –±–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Å–ª—É—á–∞–π–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
    "        batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf62443-f7e5-4e57-a353-d2b775d57b9b",
   "metadata": {},
   "source": [
    "<center>[3] ‚≠ê</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139d89a-6924-49ad-8327-75befb410ff2",
   "metadata": {},
   "source": [
    "–ê —Ç–µ–ø–µ—Ä—å —Å–æ–∑–¥–∞–π—Ç–µ –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (–Ω–∞–∑–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é <code>validation_dataloader</code>, –æ–Ω–∞ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –¥–∞–ª—å—à–µ)  \n",
    "–î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã —Å—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –ø–æ –ø–æ—Ä—è–¥–∫—É, –±–µ–∑ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a67d97-c0e1-4312-ad3e-8434dcd567b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        shuffle = False,\n",
    "        batch_size = batch_size\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7366d441-a93f-4aad-a942-ae22c46b3a25",
   "metadata": {},
   "source": [
    "<center>[3] üí´</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3a123-15fd-45bd-a099-916777f45766",
   "metadata": {},
   "source": [
    "<br>\n",
    "<H3>2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1efcc-3033-451e-ab61-8d679b70b47d",
   "metadata": {},
   "source": [
    "–°–∞–º–∞ –º–æ–¥–µ–ª—å –ª–µ–∂–∏—Ç —Ç–∞–º –∂–µ, –≥–¥–µ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä - –≤ ./models/rubert-tiny  \n",
    "–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ –¥–æ–æ–±—É—á–∞–µ–º –µ–µ –Ω–∞ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b68e1877-b26e-460d-a222-088d9a7452d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º BertForSequenceClassification. –≠—Ç–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å BERT —Å –æ–¥–∏–Ω–æ—á–Ω—ã–º –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–º —Å–ª–æ–µ–º –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"./models/rubert-tiny\",                                 # –ò—Å–ø–æ–ª—å–∑—É–µ–º rubert-tiny.\n",
    "    num_labels = 2,                                         # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Å–ª–æ—ë–≤ ‚Äì 2 –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "    output_attentions = False,                              # –ë—É–¥–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –≤–µ—Å–∞ –¥–ª—è attention-—Å–ª–æ—ë–≤. –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –Ω–µ—Ç.\n",
    "    output_hidden_states = False,                           # –ë—É–¥–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤—Å–µ—Ö —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—ë–≤. –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –Ω–µ—Ç.\n",
    ")\n",
    "\n",
    "# –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ GPU –ø–µ—Ä–µ–Ω–æ—Å–∏–º –º–æ–¥–µ–ª—å —Ç—É–¥–∞.\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f3a6a-bfd2-4f2c-8167-977d2e971083",
   "metadata": {},
   "source": [
    "<br>–ü–æ—Å–º–æ—Ç—Ä–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aba9e5ca-a512-4113-9bda-4686298c54d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 57 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (29564, 312)\n",
      "bert.embeddings.position_embeddings.weight                (512, 312)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 312)\n",
      "bert.embeddings.LayerNorm.weight                              (312,)\n",
      "bert.embeddings.LayerNorm.bias                                (312,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (312, 312)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (312,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (312, 312)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (312,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (312, 312)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (312,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (312, 312)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (312,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (312,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (312,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight            (600, 312)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                  (600,)\n",
      "bert.encoder.layer.0.output.dense.weight                  (312, 600)\n",
      "bert.encoder.layer.0.output.dense.bias                        (312,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (312,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (312,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (312, 312)\n",
      "bert.pooler.dense.bias                                        (312,)\n",
      "classifier.weight                                           (2, 312)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f949033-bb4a-44f5-9553-9c0e7cd0bcc6",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "<i>\\- –í—Å–µ –ø–æ–Ω—è—Ç–Ω–æ?  \n",
    "\\- –ù–µ—Ç!  \n",
    "\\- –ò–¥–µ–º –¥–∞–ª—å—à–µ</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedce188-0d18-4f7e-99a6-ffa1e9ad561b",
   "metadata": {},
   "source": [
    "–ü–æ–º–∏–º–æ —Å–∞–º–æ–π –º–æ–¥–µ–ª–∏, –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7876caa-4386-48f9-a6f4-901de16263a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# –ë–µ—Ä–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AdamW\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "\n",
    "# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏. –ê–≤—Ç–æ—Ä—ã BERT —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç –æ—Ç 2 –¥–æ 4.\n",
    "# –ú—ã –≤—ã–±–∏—Ä–∞–µ–º 4, –Ω–æ —É–≤–∏–¥–∏–º –ø–æ–∑–∂–µ, —á—Ç–æ —ç—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –æ–≤–µ—Ä—Ñ–∏—Ç—É –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
    "epochs = 4\n",
    "\n",
    "# –û–±—â–µ–µ —á–∏—Å–ª–æ —à–∞–≥–æ–≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —Ä–∞–≤–Ω–æ [–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π] x [—á–∏—Å–ª–æ —ç–ø–æ—Ö].\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate (LR). LR –±—É–¥–µ—Ç –ø–ª–∞–≤–Ω–æ —É–º–µ–Ω—å—à–∞—Ç—å—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf12f5e-80f9-4b68-8adc-667754f2d7fe",
   "metadata": {},
   "source": [
    "<br>\n",
    "<H3>3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd19c11-7e27-43d9-b8ca-d0c41c09eb61",
   "metadata": {},
   "source": [
    "–û–ø—Ä–µ–¥–µ–ª–∏–º, –Ω–∞ –∫–∞–∫–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –Ω–∞–º –Ω—É–∂–Ω—ã –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏: GPU –∏–ª–∏ CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d272f99-e2c4-4e79-9941-c5410e0e8f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57311f2e-8366-4302-bab2-a585c3a44141",
   "metadata": {},
   "source": [
    "<br>–î–æ–±–∞–≤–∏–º –ø–∞—Ä—É –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b188600c-aaa2-4925-94c8-fd6e23836b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏. –°—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ —Ä–µ–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –∫ –¥–∞–Ω–Ω—ã–º\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import random \n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –≤—Ä–µ–º—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –≤ —Ñ–æ—Ä–º–∞—Ç hh:mm:ss\n",
    "def format_time(elapsed):\n",
    "    # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ –±–ª–∏–∂–∞–π—à–µ–π —Å–µ–∫—É–Ω–¥—ã.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∫ hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720c9a9-0b5d-4460-87c5-ae16082150a5",
   "metadata": {},
   "source": [
    "<br>–¢–µ–ø–µ—Ä—å —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è (–¥–ª—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59d06432-54ab-4613-a90a-e66a56805947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "        device,               # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
    "        model,                # –ú–æ–¥–µ–ª—å (–≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ rubert-tiny)\n",
    "        train_dataloader,     # –ó–∞–≥—Ä—É–∑—á–∏–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "        optimizer,            # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (–≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ AdamW)\n",
    "        scheduler             # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "):\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()     # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "\n",
    "    # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –±–∞—Ç—á–∞–º –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–π 10–π –±–∞—Ç—á –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –±–∞—Ç—á–∞\n",
    "        b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        \n",
    "        # –û—á–∏—â–∞–µ–º –≤—Å–µ —Ä–∞–Ω–µ–µ –ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (—ç—Ç–æ –≤–∞–∂–Ω–æ)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –ø–æ –¥–∞–Ω–Ω—ã–º\n",
    "        out = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = out.loss\n",
    "        logits = out.logits\n",
    "        \n",
    "        # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –ø–æ –≤—Å–µ–º –±–∞—Ç—á–∞–º\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏, —á—Ç–æ–±—ã –ø–æ—Å—á–∏—Ç–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã.\n",
    "        loss.backward()\n",
    "        \n",
    "        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–æ 1.0. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã \"exploding gradients\".\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å –ø–æ–º–æ—â—å—é –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Ç–µ–∫—É—â–µ–≥–æ learning rate.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—Å–µ–º –±–∞—Ç—á–∞–º.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epo—Åh took: {:}\".format(training_time))\n",
    "    return avg_train_loss, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624f6cb-818f-4380-858b-9a37b5f89f42",
   "metadata": {},
   "source": [
    "<br>–ò —Ñ—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00f99c22-e0e2-4eb9-a5c3-91a9a77bcfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(\n",
    "        device,                  # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
    "        model,                   # –ú–æ–¥–µ–ª—å (–≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ rubert-tiny)\n",
    "        validation_dataloader    # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "):\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º evaluation: –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–ª–æ–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä dropout, –≤–µ–¥—É—Ç —Å–µ–±—è –ø–æ-–¥—Ä—É–≥–æ–º—É.\n",
    "    model.eval()\n",
    "\n",
    "    # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    \n",
    "    # –ü—Ä–æ–≥–æ–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "\n",
    "        # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–π 10–π –±–∞—Ç—á –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(validation_dataloader), elapsed))\n",
    "\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –±–∞—Ç—á–∞.\n",
    "        b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "\n",
    "        # –ì–æ–≤–æ—Ä–∏–º pytorch, —á—Ç–æ –Ω–∞–º –Ω–µ –Ω—É–∂–µ–Ω –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (–≤—Å—ë –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ)\n",
    "        with torch.no_grad():\n",
    "            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –ø–æ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.\n",
    "            out = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = out.loss\n",
    "            logits = out.logits\n",
    "\n",
    "        # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –∑–Ω–∞—á–µ–Ω–∏—è —Å GPU –Ω–∞ CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # –°—á–∏—Ç–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –±–∞—Ç—á–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –∏ –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # –í—ã–≤–æ–¥–∏–º —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –≤—Å–µ—Ö –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω—é—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –¥–ª—è –≤—Å–µ—Ö –±–∞—Ç—á–µ–π.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # –ò–∑–º–µ—Ä—è–µ–º, –∫–∞–∫ –¥–æ–ª–≥–æ —Å—á–∏—Ç–∞–ª–∞—Å—å –≤–∞–ª–∏–¥–∞—Ü–∏—è.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    return avg_val_loss, avg_val_accuracy, validation_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e4112d-6a6e-4b2c-92d9-11c92bc45d48",
   "metadata": {},
   "source": [
    "<center>[4] ‚≠ê‚≠ê</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd0dab-85a3-4dd8-ab7c-0e9dec10e8eb",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–∫—Ü–∏–∏ <code>train_step</code> –∏ <code>validation_step</code>, —Ä–µ–∞–ª–∏–∑—É–π—Ç–µ —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏  \n",
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π <code>epochs</code>  \n",
    "–î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –Ω–∞ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ —Å–¥–µ–ª–∞–π—Ç–µ –≤—ã–≤–æ–¥ –µ–µ –Ω–æ–º–µ—Ä–∞\n",
    "\n",
    "<i>–ü—Ä–∏–º.: –≤–∞—à–∞ –≥–ª–∞–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ –∑–¥–µ—Å—å - –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±–µ–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π –∏ –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–º –ø–µ—Ä–µ–¥–∞—Ç—å  \n",
    "–í—Å–µ —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è —É–∂–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–µ –Ω—É–∂–Ω–æ</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05d7d231-35a2-4d4b-853d-b4490dcac804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of    241.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    241.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    241.    Elapsed: 0:00:28.\n",
      "  Batch    40  of    241.    Elapsed: 0:00:36.\n",
      "  Batch    50  of    241.    Elapsed: 0:00:46.\n",
      "  Batch    60  of    241.    Elapsed: 0:00:55.\n",
      "  Batch    70  of    241.    Elapsed: 0:01:05.\n",
      "  Batch    80  of    241.    Elapsed: 0:01:16.\n",
      "  Batch    90  of    241.    Elapsed: 0:01:26.\n",
      "  Batch   100  of    241.    Elapsed: 0:01:36.\n",
      "  Batch   110  of    241.    Elapsed: 0:01:46.\n",
      "  Batch   120  of    241.    Elapsed: 0:01:57.\n",
      "  Batch   130  of    241.    Elapsed: 0:02:07.\n",
      "  Batch   140  of    241.    Elapsed: 0:02:17.\n",
      "  Batch   150  of    241.    Elapsed: 0:02:27.\n",
      "  Batch   160  of    241.    Elapsed: 0:02:38.\n",
      "  Batch   170  of    241.    Elapsed: 0:02:48.\n",
      "  Batch   180  of    241.    Elapsed: 0:02:57.\n",
      "  Batch   190  of    241.    Elapsed: 0:03:06.\n",
      "  Batch   200  of    241.    Elapsed: 0:03:15.\n",
      "  Batch   210  of    241.    Elapsed: 0:03:25.\n",
      "  Batch   220  of    241.    Elapsed: 0:03:33.\n",
      "  Batch   230  of    241.    Elapsed: 0:03:42.\n",
      "  Batch   240  of    241.    Elapsed: 0:03:51.\n",
      "  Average training loss: 0.61\n",
      "  Training epo—Åh took: 0:03:52\n",
      "Running Validation...\n",
      "  Batch    10  of     27.    Elapsed: 0:00:04.\n",
      "  Batch    20  of     27.    Elapsed: 0:00:08.\n",
      "  Accuracy: 0.68\n",
      "  Validation Loss: 0.64\n",
      "  Validation took: 0:00:11\n",
      "1\n",
      "  Batch    10  of    241.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    241.    Elapsed: 0:00:18.\n",
      "  Batch    30  of    241.    Elapsed: 0:00:27.\n",
      "  Batch    40  of    241.    Elapsed: 0:00:36.\n",
      "  Batch    50  of    241.    Elapsed: 0:00:46.\n",
      "  Batch    60  of    241.    Elapsed: 0:00:55.\n",
      "  Batch    70  of    241.    Elapsed: 0:01:04.\n",
      "  Batch    80  of    241.    Elapsed: 0:01:13.\n",
      "  Batch    90  of    241.    Elapsed: 0:01:22.\n",
      "  Batch   100  of    241.    Elapsed: 0:01:30.\n",
      "  Batch   110  of    241.    Elapsed: 0:01:39.\n",
      "  Batch   120  of    241.    Elapsed: 0:01:48.\n",
      "  Batch   130  of    241.    Elapsed: 0:01:57.\n",
      "  Batch   140  of    241.    Elapsed: 0:02:07.\n",
      "  Batch   150  of    241.    Elapsed: 0:02:17.\n",
      "  Batch   160  of    241.    Elapsed: 0:02:27.\n",
      "  Batch   170  of    241.    Elapsed: 0:02:37.\n",
      "  Batch   180  of    241.    Elapsed: 0:02:47.\n",
      "  Batch   190  of    241.    Elapsed: 0:02:57.\n",
      "  Batch   200  of    241.    Elapsed: 0:03:07.\n",
      "  Batch   210  of    241.    Elapsed: 0:03:17.\n",
      "  Batch   220  of    241.    Elapsed: 0:03:27.\n",
      "  Batch   230  of    241.    Elapsed: 0:03:37.\n",
      "  Batch   240  of    241.    Elapsed: 0:03:47.\n",
      "  Average training loss: 0.60\n",
      "  Training epo—Åh took: 0:03:47\n",
      "Running Validation...\n",
      "  Batch    10  of     27.    Elapsed: 0:00:05.\n",
      "  Batch    20  of     27.    Elapsed: 0:00:09.\n",
      "  Accuracy: 0.68\n",
      "  Validation Loss: 0.63\n",
      "  Validation took: 0:00:12\n",
      "1\n",
      "  Batch    10  of    241.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    241.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    241.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    241.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    241.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    241.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    241.    Elapsed: 0:01:09.\n",
      "  Batch    80  of    241.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    241.    Elapsed: 0:01:29.\n",
      "  Batch   100  of    241.    Elapsed: 0:01:40.\n",
      "  Batch   110  of    241.    Elapsed: 0:01:50.\n",
      "  Batch   120  of    241.    Elapsed: 0:01:59.\n",
      "  Batch   130  of    241.    Elapsed: 0:02:09.\n",
      "  Batch   140  of    241.    Elapsed: 0:02:19.\n",
      "  Batch   150  of    241.    Elapsed: 0:02:29.\n",
      "  Batch   160  of    241.    Elapsed: 0:02:38.\n",
      "  Batch   170  of    241.    Elapsed: 0:02:48.\n",
      "  Batch   180  of    241.    Elapsed: 0:02:59.\n",
      "  Batch   190  of    241.    Elapsed: 0:03:12.\n",
      "  Batch   200  of    241.    Elapsed: 0:03:23.\n",
      "  Batch   210  of    241.    Elapsed: 0:03:32.\n",
      "  Batch   220  of    241.    Elapsed: 0:03:43.\n",
      "  Batch   230  of    241.    Elapsed: 0:03:57.\n",
      "  Batch   240  of    241.    Elapsed: 0:04:07.\n",
      "  Average training loss: 0.59\n",
      "  Training epo—Åh took: 0:04:08\n",
      "Running Validation...\n",
      "  Batch    10  of     27.    Elapsed: 0:00:04.\n",
      "  Batch    20  of     27.    Elapsed: 0:00:08.\n",
      "  Accuracy: 0.68\n",
      "  Validation Loss: 0.63\n",
      "  Validation took: 0:00:11\n",
      "1\n",
      "  Batch    10  of    241.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    241.    Elapsed: 0:00:18.\n",
      "  Batch    30  of    241.    Elapsed: 0:00:27.\n",
      "  Batch    40  of    241.    Elapsed: 0:00:36.\n",
      "  Batch    50  of    241.    Elapsed: 0:00:45.\n",
      "  Batch    60  of    241.    Elapsed: 0:00:54.\n",
      "  Batch    70  of    241.    Elapsed: 0:01:03.\n",
      "  Batch    80  of    241.    Elapsed: 0:01:11.\n",
      "  Batch    90  of    241.    Elapsed: 0:01:21.\n",
      "  Batch   100  of    241.    Elapsed: 0:01:29.\n",
      "  Batch   110  of    241.    Elapsed: 0:01:38.\n",
      "  Batch   120  of    241.    Elapsed: 0:01:47.\n",
      "  Batch   130  of    241.    Elapsed: 0:01:56.\n",
      "  Batch   140  of    241.    Elapsed: 0:02:05.\n",
      "  Batch   150  of    241.    Elapsed: 0:02:14.\n",
      "  Batch   160  of    241.    Elapsed: 0:02:23.\n",
      "  Batch   170  of    241.    Elapsed: 0:02:32.\n",
      "  Batch   180  of    241.    Elapsed: 0:02:41.\n",
      "  Batch   190  of    241.    Elapsed: 0:02:50.\n",
      "  Batch   200  of    241.    Elapsed: 0:03:00.\n",
      "  Batch   210  of    241.    Elapsed: 0:03:10.\n",
      "  Batch   220  of    241.    Elapsed: 0:03:19.\n",
      "  Batch   230  of    241.    Elapsed: 0:03:28.\n",
      "  Batch   240  of    241.    Elapsed: 0:03:37.\n",
      "  Average training loss: 0.59\n",
      "  Training epo—Åh took: 0:03:38\n",
      "Running Validation...\n",
      "  Batch    10  of     27.    Elapsed: 0:00:04.\n",
      "  Batch    20  of     27.    Elapsed: 0:00:08.\n",
      "  Accuracy: 0.68\n",
      "  Validation Loss: 0.63\n",
      "  Validation took: 0:00:11\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "epochs = 0\n",
    "for i in range(4):\n",
    "    epochs = 1\n",
    "    i\n",
    "    train_step(device, model, train_dataloader, optimizer, scheduler)\n",
    "    validation_step(device, model, validation_dataloader)\n",
    "    print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cae0c0-adda-47d2-86e8-7375ba341a08",
   "metadata": {},
   "source": [
    "<center>[4] üí´</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932be48f-0a83-4bcd-8b6c-e3399a6b0793",
   "metadata": {},
   "source": [
    "<H3>4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea5f56-51e5-49dc-83be-79d008496dd1",
   "metadata": {},
   "source": [
    "–î–∞–≤–∞–π—Ç–µ —Ç–µ–ø–µ—Ä—å –≤–µ—Ä–Ω–µ–º—Å—è –Ω–µ–º–Ω–æ–≥–æ –Ω–∞–∑–∞–¥ –∏ –≤—Å–ø–æ–º–Ω–∏–º, —á—Ç–æ –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ <i>–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é</i> –º–æ–¥–µ–ª—å  \n",
    "–≠—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ –µ–µ —É–∂–µ –æ–±—É—á–∞–ª–∏ —Ä–∞–Ω–µ–µ, –ø–æ–ª—É—á–∏–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –∑–∞–ø–∏—Å–∞–ª–∏ –∏—Ö, –∞ –º—ã –∏—Ö —Å—á–∏—Ç–∞–ª–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db7904-3cbf-4749-af6e-1947f2648618",
   "metadata": {},
   "source": [
    "–ù–∞—à—É –º–æ–¥–µ–ª—å —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–º–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å  \n",
    "–¢–æ–≥–¥–∞ –≤ —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑ –º—ã —Å–º–æ–∂–µ–º –µ–µ —Å—á–∏—Ç–∞—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, –Ω–µ –æ–±—É—á–∞—è –∑–∞–Ω–æ–≤–æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ade7ae33-aeb8-45c1-a84f-5a3ce0102d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./models/rubert-tiny-trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/rubert-tiny-trained\\\\tokenizer_config.json',\n",
       " './models/rubert-tiny-trained\\\\special_tokens_map.json',\n",
       " './models/rubert-tiny-trained\\\\vocab.txt',\n",
       " './models/rubert-tiny-trained\\\\added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# –ó–∞–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –º–æ–¥–µ–ª–∏\n",
    "model_dir = './models/rubert-tiny-trained'\n",
    "# –ï—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Å–æ–∑–¥–∞–µ–º –µ—ë\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % model_dir)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ –µ—ë —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä.\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c866266-1239-4db7-ad86-646c49ee33e4",
   "metadata": {},
   "source": [
    "<center>[5] ‚≠ê‚≠ê‚≠ê</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffe08b4-0e5f-457f-9ba4-9ad9efa50149",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∑–∞–Ω–æ–≤–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –µ–µ –∫ –¥–∞—Ç–∞—Å–µ—Ç—É CoLA –∏ –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫—É accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe1af55c-8f3f-4f76-bcf7-8704b555f39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation...\n",
      "  Batch    10  of    268.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    268.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    268.    Elapsed: 0:00:12.\n",
      "  Batch    40  of    268.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    268.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    268.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    268.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    268.    Elapsed: 0:00:33.\n",
      "  Batch    90  of    268.    Elapsed: 0:00:37.\n",
      "  Batch   100  of    268.    Elapsed: 0:00:42.\n",
      "  Batch   110  of    268.    Elapsed: 0:00:47.\n",
      "  Batch   120  of    268.    Elapsed: 0:00:51.\n",
      "  Batch   130  of    268.    Elapsed: 0:00:56.\n",
      "  Batch   140  of    268.    Elapsed: 0:01:00.\n",
      "  Batch   150  of    268.    Elapsed: 0:01:04.\n",
      "  Batch   160  of    268.    Elapsed: 0:01:08.\n",
      "  Batch   170  of    268.    Elapsed: 0:01:13.\n",
      "  Batch   180  of    268.    Elapsed: 0:01:16.\n",
      "  Batch   190  of    268.    Elapsed: 0:01:20.\n",
      "  Batch   200  of    268.    Elapsed: 0:01:24.\n",
      "  Batch   210  of    268.    Elapsed: 0:01:28.\n",
      "  Batch   220  of    268.    Elapsed: 0:01:32.\n",
      "  Batch   230  of    268.    Elapsed: 0:01:36.\n",
      "  Batch   240  of    268.    Elapsed: 0:01:40.\n",
      "  Batch   250  of    268.    Elapsed: 0:01:44.\n",
      "  Batch   260  of    268.    Elapsed: 0:01:48.\n",
      "  Accuracy: 0.70\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:01:51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5906370555731788, 0.7039745469083156, '0:01:51')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"./models/rubert-tiny-trained\",                                 # –ò—Å–ø–æ–ª—å–∑—É–µ–º rubert-tiny.\n",
    "    num_labels = 2,                                         # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Å–ª–æ—ë–≤ ‚Äì 2 –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "    output_attentions = False,                              # –ë—É–¥–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –≤–µ—Å–∞ –¥–ª—è attention-—Å–ª–æ—ë–≤. –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –Ω–µ—Ç.\n",
    "    output_hidden_states = False,                           # –ë—É–¥–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤—Å–µ—Ö —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—ë–≤. –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –Ω–µ—Ç.\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"./data/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "sentences = df['sentence'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "encoded_dict_rew = tokenizer.batch_encode_plus(\n",
    "    list(rew),                           # –¢–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.\n",
    "    add_special_tokens=True,                   # –î–æ–±–∞–≤–ª—è–µ–º '[CLS]' –∏ '[SEP]'\n",
    "    max_length=64,                             # –î–æ–ø–æ–ª–Ω—è–µ–º [PAD] –∏–ª–∏ –æ–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ 64 —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–∞–∫–∂–µ attn. masks.\n",
    "    return_tensors='pt',                       # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –≤–∏–¥–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤ pytorch.\n",
    ")\n",
    "\n",
    "input_ids = encoded_dict['input_ids']\n",
    "attention_masks = encoded_dict['attention_mask']\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset_val1 = TensorDataset(input_ids, attention_masks, labels)\n",
    "validation_dataloader1 = DataLoader(dataset_val1, shuffle=False, batch_size = batch_size)\n",
    "\n",
    "validation_step(device, model, validation_dataloader1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448c6ae-3194-45e3-b2f3-7a4aaaf90539",
   "metadata": {},
   "source": [
    "<center>[5] üí´</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c55f6-f4dc-46e2-b14c-56752a930922",
   "metadata": {},
   "source": [
    "<H2>–ó–∞–¥–∞–Ω–∏–µ –Ω–∞ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—É—é —Ä–∞–±–æ—Ç—É</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe04169-8116-4cd4-9a0d-a1b1d5b7577c",
   "metadata": {},
   "source": [
    "<b>1. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç —Å –æ—Ç–∑—ã–≤–∞–º–∏ –Ω–∞ —Ñ–∏–ª—å–º—ã (./data/train.csv)\n",
    "\n",
    "–ü—Ä–∏–º.:\n",
    "- –í –ø—Ä–∏–º–µ—Ä–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–∞–±–ª–∏—Ü–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º <code>\\t</code>, –∑–¥–µ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å <code>,</code>\n",
    "- –ó–Ω–∞—á–µ–Ω–∏—è <code>label</code> –ø—Ä–∏–¥–µ—Ç—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∫ —á–∏—Å–ª–æ–≤–æ–º—É –≤–∏–¥—É —Å –ø–æ–º–æ—â—å—é <code>LabelEncoder</code> (—Å–º. –ø—Ä–µ–¥—ã–¥—É—â—É—é –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—É—é —Ä–∞–±–æ—Ç—É)\n",
    "\n",
    "<b>2. –û–±—É—á–∏—Ç—å rubert-tiny –∏–ª–∏ –¥—Ä—É–≥—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —Ç–∏–ø –æ—Ç–∑—ã–≤–∞ (–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π –∏–ª–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π)  \n",
    "<b>3. –ü—Ä–∏–º–µ–Ω–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º (./data/test.csv)\n",
    "\n",
    "–ü—Ä–∏–º.:\n",
    "- –î–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π <code>TensorDataset</code> –∏ <code>DataLoader</code> –±–µ–∑ <code>label</code>\n",
    "- –î–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ —Ñ—É–Ω–∫—Ü–∏—é –≤–∞–ª–∏–¥–∞—Ü–∏–∏)\n",
    "\n",
    "<b>4. –ó–∞–ø–∏—Å–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –±–æ—Ç—É\n",
    "\n",
    "–ü—Ä–∏–º.:\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–º–µ—Ä ./data/sample_submission.csv\n",
    "- –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º–æ–¥–µ–ª—å –≤—ã–¥–∞—Å—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–∑—ã–≤–∞ 2 –∑–Ω–∞—á–µ–Ω–∏—è - –µ–≥–æ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –∫–ª–∞—Å—Å–∞–º \"–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π\" –∏ \"–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π\" –≤ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏; –≤–∞–º –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏—Ö –≤ –æ–¥–Ω–æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a7e45-9821-4270-9ec5-4f311eb5e50f",
   "metadata": {},
   "source": [
    "<b>–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞ –∑–∞—â–∏—Ç—É –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è:\n",
    "- —Ç–æ–ø-5 –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ –≥—Ä—É–ø–ø—ã –∏ score>0.55 - 10 –±–∞–ª–ª–æ–≤\n",
    "- —Ç–æ–ø-10 –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ –≥—Ä—É–ø–ø—ã –∏ score>0.4 - 8 –±–∞–ª–ª–æ–≤\n",
    "- score>0.2 - 5 –±–∞–ª–ª–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3309ff8-b6f1-46f7-9209-eea68c4b94fd",
   "metadata": {},
   "source": [
    "<i>–ü—Ä–∏–º.: –º–µ—Å—Ç–æ –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ —Å—á–∏—Ç–∞–µ—Ç—Å—è –Ω–∞ –º–æ–º–µ–Ω—Ç —Å–¥–∞—á–∏  \n",
    "–î—É–º–∞–π—Ç–µ =)</i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
